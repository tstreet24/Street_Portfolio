{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tstreet24/Street_Portfolio/blob/main/SlotMarketSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_tuner\n",
        "!pip install arch"
      ],
      "metadata": {
        "id": "XINL6J-pDQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-HKl_89azAf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDD9HClrWEyG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from kerastuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from arch import arch_model\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4658EVeWebB"
      },
      "source": [
        "# **INITIAL CODE TO GET PAST TWO YEARS OF DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8Vr3eFEWAQ0"
      },
      "outputs": [],
      "source": [
        "# Function to fetch tickers\n",
        "def fetch_sp500_tickers():\n",
        "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "    tables = pd.read_html(url)\n",
        "    sp500_table = tables[0]\n",
        "    sp500_tickers = sp500_table['Symbol'].tolist()\n",
        "    return sp500_tickers\n",
        "\n",
        "sp500_tickers = fetch_sp500_tickers()\n",
        "print(sp500_tickers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ6INSijLLTp"
      },
      "outputs": [],
      "source": [
        "# Pulling closing prices from beginning of 2022 to february 2024\n",
        "sp500_tickers = fetch_sp500_tickers()\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime(2022, 1, 1)\n",
        "\n",
        "all_closing_prices = pd.DataFrame()\n",
        "\n",
        "for ticker in sp500_tickers:\n",
        "    stock = yf.Ticker(ticker)\n",
        "    hist = stock.history(start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))\n",
        "    all_closing_prices[ticker] = hist['Close']\n",
        "\n",
        "print(all_closing_prices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_7zSRmQYw8e"
      },
      "outputs": [],
      "source": [
        "# Dropping a column that was NA\n",
        "all_closing_prices = all_closing_prices.dropna(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQyhp7DYWlMH"
      },
      "outputs": [],
      "source": [
        "all_closing_prices.to_csv(\"closing_prices.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2xnSyqlafGI"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ioVzt8ap4u"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/closing_prices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upn_Vg8e0VBz"
      },
      "outputs": [],
      "source": [
        "# Normalize (min-max)\n",
        "df.iloc[:, 1:] = df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce') # don't apply to 0th column, which is date\n",
        "normalized_df = df.copy()\n",
        "normalized_df.iloc[:, 1:] = (df.iloc[:, 1:] - df.iloc[:, 1:].min()) / (df.iloc[:, 1:].max() - df.iloc[:, 1:].min())\n",
        "normalized_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiA2G1Gy0lki"
      },
      "outputs": [],
      "source": [
        "# Filter data to 2023 points\n",
        "normalized_df['Date'] = pd.to_datetime(normalized_df['Date'], utc = True)\n",
        "df_2023 = normalized_df[(normalized_df['Date'].dt.year == 2023) | (normalized_df['Date'].dt.year == 2022)]\n",
        "df_2024_first_entry = normalized_df[normalized_df['Date'].dt.year == 2024].head(1)\n",
        "df_initial = pd.concat([df_2023, df_2024_first_entry])\n",
        "df_initial.tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Functions"
      ],
      "metadata": {
        "id": "uMD4DAcmnTK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to properly shape X and y for LSTM\n",
        "def create_sequences(series, lag):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(len(series) - lag):\n",
        "        inputs.append(series[i:(i + lag)])\n",
        "        targets.append(series[i + lag])\n",
        "    return np.array(inputs), np.array(targets)"
      ],
      "metadata": {
        "id": "aGoEmy0KnR9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform walk forward validation with varying parameters\n",
        "def walk_forward_validation(series, lag, units, dropout, val_steps, epochs):\n",
        "    series = np.array(series) if not isinstance(series, np.ndarray) else series\n",
        "\n",
        "    X, y = create_sequences(series, lag)\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    split_idx = len(X) - val_steps\n",
        "\n",
        "    mses = []\n",
        "\n",
        "    # Define architecture\n",
        "    input_layer = keras.Input(shape=(lag, 1))\n",
        "    lstm_layer = keras.layers.LSTM(units, activation='relu')(input_layer)\n",
        "    dropout_layer = keras.layers.Dropout(dropout)(lstm_layer)\n",
        "    output_layer = keras.layers.Dense(1)(dropout_layer)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "\n",
        "    # Validation loop\n",
        "    for step in range(val_steps - 1):\n",
        "        X_train, y_train = X[:split_idx + step], y[:split_idx + step]\n",
        "        X_val, y_val = X[split_idx + step:split_idx + step + 1], y[split_idx + step:split_idx + step + 1]\n",
        "\n",
        "\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=0)\n",
        "\n",
        "        mse = model.evaluate(X_val, y_val, verbose=0)[1]  # [1] for mse\n",
        "        mses.append(mse)\n",
        "\n",
        "    # Return the MSEs for each validation step and the final model\n",
        "    return mses, model\n"
      ],
      "metadata": {
        "id": "6KV81qninWPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS5Ynia75fDo"
      },
      "source": [
        "# Model Building and Evaluation, ONE STOCK for EXPLORATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1L0yh-T52rx"
      },
      "outputs": [],
      "source": [
        "series = df_initial.iloc[:,1]\n",
        "lag = 10\n",
        "X, y = create_sequences(series, lag)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_M7GLNm8ADI"
      },
      "outputs": [],
      "source": [
        "X_train = X[:-1]\n",
        "X_test = X[-1:]\n",
        "y_train = y[:-1]\n",
        "y_test = y[-1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqZ35M-moh0Y"
      },
      "outputs": [],
      "source": [
        "input = keras.Input(shape=(lag, 1))\n",
        "lstm = keras.layers.LSTM(30, activation='relu')(input)\n",
        "dropout = keras.layers.Dropout(0.2)(lstm)\n",
        "output = keras.layers.Dense(1)(dropout)\n",
        "model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics = ['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqer3niDsraq"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train,\n",
        "          y_train,\n",
        "          epochs=20,\n",
        "          batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H77ZnhaL8sBX"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT - Determining adequate number of epochs"
      ],
      "metadata": {
        "id": "tXv4geS3MMVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform epoch experiment with walk forward validation\n",
        "def epoch_experiment(df, lag, units, dropout, val_steps, epoch_values):\n",
        "\n",
        "    # Dictionary with results\n",
        "    results = {column: {} for column in df.columns}\n",
        "\n",
        "    for column in df.columns:\n",
        "        print(f\"Processing column: {column}\")\n",
        "        series = df[column].values\n",
        "\n",
        "        for epochs in epoch_values:\n",
        "            mses, _ = walk_forward_validation(series, lag, units, dropout, val_steps, epochs)\n",
        "            avg_mse = np.mean(mses)\n",
        "            results[column][epochs] = avg_mse\n",
        "\n",
        "    # Plotting\n",
        "    for column in df.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        epochs_list = list(results[column].keys())\n",
        "        avg_mses = list(results[column].values())\n",
        "\n",
        "        model_params_label = f'Walk Forward Validation MSE - Lag: {lag}, Units: {units}, Dropout: {dropout}'\n",
        "\n",
        "        plt.plot(epochs_list, avg_mses, marker='o', label=model_params_label)\n",
        "\n",
        "        plt.title(f'Walk Forward Validation MSE by Epoch for {column}')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Average MSE')\n",
        "        plt.xticks(epochs_list)\n",
        "        plt.grid(False)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ymw1TxqhMPb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for experiment\n",
        "lag = 10\n",
        "units = 30\n",
        "dropout = 0.2\n",
        "val_steps = 8\n",
        "epoch_range = range(1,21)\n",
        "\n",
        "# Randomly pick columns (TSLA, UNH, ETR)\n",
        "np.random.seed(15773)\n",
        "random_num = np.random.choice(range(1, df_initial.shape[1] + 1), 3, replace=False)\n",
        "df = df_initial.iloc[:,[random_num[0]\n",
        "                        ,random_num[1]\n",
        "                        ,random_num[2]\n",
        "                        ]]\n",
        "\n",
        "# Call function\n",
        "epoch_experiment(df, lag, units, dropout, val_steps, epoch_range)"
      ],
      "metadata": {
        "id": "B7nW_ZQbPZzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion: Around 5 epochs seems reasonable"
      ],
      "metadata": {
        "id": "XVoffNdqlza-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT - Determine lag length"
      ],
      "metadata": {
        "id": "DMwSKZ9HxYqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform lag experiment using walk forward validation\n",
        "def lag_experiment(df, lag_values, units, dropout, val_steps, constant_epochs):\n",
        "    # List to store all results\n",
        "    results = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        print(f\"Processing column: {column}\")\n",
        "        series = df[column].values\n",
        "\n",
        "        for lag in lag_values:\n",
        "            mses, _ = walk_forward_validation(series, lag, units, dropout, val_steps, constant_epochs)\n",
        "            avg_mse = np.mean(mses)\n",
        "\n",
        "            results.append({\n",
        "                'Column': column,\n",
        "                'Lag': lag,\n",
        "                'Average_MSE': avg_mse\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "TOfMtDqvxcG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for experiment\n",
        "lag_values = [7, 14, 28]\n",
        "units = 30\n",
        "dropout = 0.2\n",
        "val_steps = 8\n",
        "epochs = 5\n",
        "\n",
        "# Randomly pick columns\n",
        "np.random.seed(15773)\n",
        "random_num = np.random.choice(range(1, df_initial.shape[1] + 1), 3, replace=False)\n",
        "df = df_initial.iloc[:,[random_num[0]\n",
        "                        ,random_num[1]\n",
        "                        ,random_num[2]\n",
        "                        ]]\n",
        "\n",
        "# Call function\n",
        "results = lag_experiment(df, lag_values, units, dropout, val_steps, epochs)\n",
        "lag_agg = results.groupby('Lag')['Average_MSE'].mean().reset_index()\n",
        "lag_agg"
      ],
      "metadata": {
        "id": "6tdmrcvtyWHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION: We will use a lag of 7 herein"
      ],
      "metadata": {
        "id": "HlU86zRV1A-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT - Parameter tuning with keras.tuner, no walk forward validation, for warm start"
      ],
      "metadata": {
        "id": "G1YO3hmXHF8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - TSLA\n",
        "series = df.iloc[:,0]\n",
        "lag = 7\n",
        "X, y = create_sequences(series, lag)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "X_train = X[:-1]\n",
        "X_test = X[-1:]\n",
        "y_train = y[:-1]\n",
        "y_test = y[-1:]\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    input_layer = keras.Input(shape=(lag, 1))\n",
        "    lstm_layer = keras.layers.LSTM(\n",
        "        hp.Int('lstm_units', min_value=10, max_value=100, step=10), activation='relu')(input_layer)\n",
        "    dropout_layer = keras.layers.Dropout(\n",
        "        hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.05))(lstm_layer)\n",
        "    output_layer = keras.layers.Dense(1)(dropout_layer)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    project_name='tuner0'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=5, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "id": "hB2T12JOHLcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - TSLA\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "_LPEywShHxnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - UNH\n",
        "series = df.iloc[:,1]\n",
        "lag = 7\n",
        "X, y = create_sequences(series, lag)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "X_train = X[:-1]\n",
        "X_test = X[-1:]\n",
        "y_train = y[:-1]\n",
        "y_test = y[-1:]\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    input_layer = keras.Input(shape=(lag, 1))\n",
        "    lstm_layer = keras.layers.LSTM(\n",
        "        hp.Int('lstm_units', min_value=10, max_value=100, step=10), activation='relu')(input_layer)\n",
        "    dropout_layer = keras.layers.Dropout(\n",
        "        hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.05))(lstm_layer)\n",
        "    output_layer = keras.layers.Dense(1)(dropout_layer)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    project_name='tuner1'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=5, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "id": "vYVUJLwp0v_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - UNH\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "t3xUFR_I00cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - ETR\n",
        "series = df.iloc[:,2]\n",
        "lag = 7\n",
        "X, y = create_sequences(series, lag)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "X_train = X[:-1]\n",
        "X_test = X[-1:]\n",
        "y_train = y[:-1]\n",
        "y_test = y[-1:]\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    input_layer = keras.Input(shape=(lag, 1))\n",
        "    lstm_layer = keras.layers.LSTM(\n",
        "        hp.Int('lstm_units', min_value=10, max_value=100, step=10), activation='relu')(input_layer)\n",
        "    dropout_layer = keras.layers.Dropout(\n",
        "        hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.05))(lstm_layer)\n",
        "    output_layer = keras.layers.Dense(1)(dropout_layer)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    project_name='tuner2'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=5, validation_data = (X_test, y_test))"
      ],
      "metadata": {
        "id": "NeSg8gsu0285"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - ETR\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "tJkO5DAb05If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION - for stock 0, 90 units and 10% dropout   ;    for stock 1, 100 units and 0% dropout    ;    for stock 2, 50 units and 15% dropout"
      ],
      "metadata": {
        "id": "mI24T1Rh1qWw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8hrBPKw6CAA"
      },
      "source": [
        "# EXPERIMENT - Manual parameter tuning for units and dropout rate using warm starts from previous experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk6kR-tV6Ejy"
      },
      "outputs": [],
      "source": [
        "# Function to perform manual parameter tuning\n",
        "def parameter_tuning(df, lag, unit_range, dropout_range, val_steps, epochs):\n",
        "    # Initialize a list to store the results\n",
        "    all_results = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        series = df[column].values\n",
        "\n",
        "        for units in unit_range:\n",
        "\n",
        "            for dropout in dropout_range:\n",
        "                mses, _ = walk_forward_validation(series, lag, units, dropout, val_steps, epochs)\n",
        "                avg_mse = np.mean(mses)\n",
        "\n",
        "                all_results.append({\n",
        "                    'Column': column,\n",
        "                    'Units': units,\n",
        "                    'Dropout': dropout,\n",
        "                    'Average_MSE': avg_mse\n",
        "                })\n",
        "\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tp7fpmU_i9a"
      },
      "outputs": [],
      "source": [
        "# Parameters for experiment\n",
        "lag = 7\n",
        "unit_range = [80, 90, 100, 110]\n",
        "dropout_range = [0, 0.05, 0.1, 0.15]\n",
        "val_steps = 8\n",
        "epochs = 5\n",
        "\n",
        "# Randomly pick columns\n",
        "np.random.seed(15773)\n",
        "random_num = np.random.choice(range(1, df_initial.shape[1] + 1), 3, replace=False)\n",
        "df = df_initial.iloc[:,[random_num[0]\n",
        "                        ,random_num[1]\n",
        "                        ,random_num[2]\n",
        "                        ]]\n",
        "\n",
        "# Call function\n",
        "results = parameter_tuning(df, lag, unit_range, dropout_range, val_steps, epochs)\n",
        "lag_agg = results.groupby(['Units', 'Dropout'])['Average_MSE'].mean().reset_index()\n",
        "lag_agg.rename(columns={'Average_MSE': 'Aggregated_Average_MSE'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFCaSZoZGY9k"
      },
      "outputs": [],
      "source": [
        "lag_agg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONCLUSION - 90 units, 15% dropout rate"
      ],
      "metadata": {
        "id": "9rKTvfzK7ifj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT - Stacking LSTMS"
      ],
      "metadata": {
        "id": "MnCB4biB5-HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create architecture based on number of stacks\n",
        "def build_stacks(lag, units, dropout, num_stacks):\n",
        "    input_layer = keras.Input(shape=(lag, 1))\n",
        "    x = input_layer\n",
        "\n",
        "    # Adjust units based on the number of stacks\n",
        "    adjusted_units = int(units / num_stacks)\n",
        "\n",
        "    for _ in range(num_stacks):\n",
        "        x = keras.layers.LSTM(adjusted_units, activation='relu', return_sequences=(_ < num_stacks - 1))(x)\n",
        "        x = keras.layers.Dropout(dropout)(x)\n",
        "\n",
        "    output_layer = keras.layers.Dense(1)(x)\n",
        "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "H5qcVTFY6AXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform stacks experiment using walk forward validation\n",
        "def evaluate_stacked_models(df, lag, units, dropout, val_steps, epochs, stack_options):\n",
        "    results = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        series = df[column].values\n",
        "\n",
        "        for num_stacks in stack_options:\n",
        "            print(f\"Evaluating: {column} with {num_stacks} Stacks\")\n",
        "            model = build_stacks(lag, units, dropout, num_stacks)\n",
        "\n",
        "            mses, _ = walk_forward_validation(series, lag, units, dropout, val_steps, epochs)\n",
        "            avg_mse = np.mean(mses)\n",
        "\n",
        "            results.append({\n",
        "                'Column': column,\n",
        "                'Num_Stacks': num_stacks,\n",
        "                'Average_MSE': avg_mse\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "W8VbF1e17yJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for experiment\n",
        "lag = 7\n",
        "units = 90\n",
        "dropout = 0.15\n",
        "val_steps = 8\n",
        "epochs = 5\n",
        "stack_options = range(1, 11)\n",
        "\n",
        "# Randomly pick columns\n",
        "np.random.seed(15773)\n",
        "random_num = np.random.choice(range(1, df_initial.shape[1] + 1), 3, replace=False)\n",
        "df = df_initial.iloc[:,[random_num[0]\n",
        "                        ,random_num[1]\n",
        "                        ,random_num[2]\n",
        "                        ]]\n",
        "\n",
        "# Call function\n",
        "results = evaluate_stacked_models(df, lag, units, dropout, val_steps, epochs, stack_options)\n",
        "lag_agg = results.groupby(['Num_Stacks'])['Average_MSE'].mean().reset_index()\n",
        "lag_agg.rename(columns={'Average_MSE': 'Aggregated_Average_MSE'}, inplace=True)"
      ],
      "metadata": {
        "id": "2I6T72OG78hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lag_agg"
      ],
      "metadata": {
        "id": "0iEHqJOv8WJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion: 6 stacks are best"
      ],
      "metadata": {
        "id": "VgFAkaCb9tC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating models"
      ],
      "metadata": {
        "id": "f7dBWaaCJtGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_save_path = '/content/drive/My Drive/HODL_Project/Models/'\n",
        "\n",
        "for i in range(498, 498):\n",
        "  series = df_initial.iloc[:,i]\n",
        "  lag = 7\n",
        "  X, y = create_sequences(series, lag)\n",
        "  X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "  X_train = X[:-1]\n",
        "  X_test = X[-1:]\n",
        "  y_train = y[:-1]\n",
        "  y_test = y[-1:]\n",
        "\n",
        "  input = keras.Input(shape=(lag, 1))\n",
        "  lstm_1 = keras.layers.LSTM(15, activation='relu', return_sequences=True)(input)\n",
        "  dropout_1 = keras.layers.Dropout(0.15)(lstm_1)\n",
        "  lstm_2 = keras.layers.LSTM(15, activation='relu', return_sequences=True)(dropout_1)\n",
        "  dropout_2 = keras.layers.Dropout(0.15)(lstm_2)\n",
        "  lstm_3 = keras.layers.LSTM(15, activation='relu', return_sequences=True)(dropout_2)\n",
        "  dropout_3 = keras.layers.Dropout(0.15)(lstm_3)\n",
        "  lstm_4 = keras.layers.LSTM(15, activation='relu', return_sequences=True)(dropout_3)\n",
        "  dropout_4 = keras.layers.Dropout(0.15)(lstm_4)\n",
        "  lstm_5 = keras.layers.LSTM(15, activation='relu', return_sequences=True)(dropout_4)\n",
        "  dropout_5 = keras.layers.Dropout(0.15)(lstm_5)\n",
        "  lstm_6 = keras.layers.LSTM(15, activation='relu')(dropout_5)\n",
        "  dropout_6 = keras.layers.Dropout(0.15)(lstm_6)\n",
        "  output = keras.layers.Dense(1)(dropout_6)\n",
        "  model = keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics = ['mse'])\n",
        "\n",
        "\n",
        "  model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0, validation_data=(X_test, y_test))\n",
        "\n",
        "  model_save_path = os.path.join(base_save_path, f'model_column_{i}.h5')\n",
        "  model.save(model_save_path)\n",
        "  print(f'Model saved to {model_save_path}')\n",
        "\n"
      ],
      "metadata": {
        "id": "5l4ItZZzI2-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GARCH MODELS"
      ],
      "metadata": {
        "id": "XvBv7mMrHTmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "gVTqywdAHU62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], utc = True)\n",
        "df_2023 = df[(df['Date'].dt.year == 2023) | (df['Date'].dt.year == 2022)]\n",
        "df_2024_first_entry = df[df['Date'].dt.year == 2024].head(1)\n",
        "df = pd.concat([df_2023, df_2024_first_entry])"
      ],
      "metadata": {
        "id": "IlUELpHBHV0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_returns = np.log(df.iloc[:, 1:] / df.iloc[:, 1:].shift(1))\n",
        "log_returns = log_returns.dropna()\n",
        "df = log_returns"
      ],
      "metadata": {
        "id": "4fARacbfJWeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Zd8coPAAKSkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_save_path = '/content/drive/My Drive/HODL_Project/GARCH_Models/'\n",
        "for i in range(0,df.shape[1]):  # Start=1 for naming files starting from 1\n",
        "    # Select the series excluding the last row\n",
        "    series = df.iloc[:,i]\n",
        "\n",
        "    # Fit a GARCH(1,1) model\n",
        "    model = arch_model(series, vol='Garch', p=1, q=1, mean='constant', dist='Normal')\n",
        "    results = model.fit(disp='off')\n",
        "\n",
        "    # Construct the save path for the model\n",
        "    model_save_path = os.path.join(base_save_path, f'garch_model_{i+1}.pkl')\n",
        "\n",
        "    # Save the model using pickle\n",
        "    with open(model_save_path, 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "-LKClyfdH3k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Evaluation"
      ],
      "metadata": {
        "id": "1WdU7rAYsa-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "naQyi9JxsjZo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xi867Z_PsgCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_model(model, train_series, test_series):\n",
        "  norm_preds = []\n",
        "  mse = []\n",
        "\n",
        "  train_series = np.array(train_series)\n",
        "  test_series = np.array(test_series)\n",
        "\n",
        "  size_test = len(test_series)\n",
        "\n",
        "  for i in range(1,size_test):\n",
        "\n",
        "      #Prep Data\n",
        "      model_series = np.append(train_series, test_series[0:i+1])\n",
        "\n",
        "      #Prep data for model\n",
        "      X, y = create_sequences(model_series, 7)\n",
        "      X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "      X_train = X[:-1]\n",
        "      X_test = X[-1:]\n",
        "      y_train = y[:-1]\n",
        "      y_test = y[-1:]\n",
        "\n",
        "      #Update model\n",
        "      updated_model = model.fit(X_train, y_train, epochs=5, batch_size=32, verbose = 0)\n",
        "      # Predict the stock price for the next day\n",
        "      pred_norm_price = model.predict(X_test)[0][0]\n",
        "      #mse\n",
        "      y_test = y_test[0]\n",
        "      error = np.square(y_test - pred_norm_price)\n",
        "\n",
        "      # Store the prediction & error\n",
        "      norm_preds.append(pred_norm_price)\n",
        "      mse.append(error)\n",
        "\n",
        "      #Save Model if last run\n",
        "      if i == size_test - 1:\n",
        "        model_filename = f'/content/drive/My Drive/HODL_Project/Updated_LSTM/model_column_{j}.h5'\n",
        "        model.save(model_filename)\n",
        "\n",
        "  #return dataframe\n",
        "  norm_preds = pd.DataFrame(norm_preds)\n",
        "  mse = pd.DataFrame(mse)\n",
        "\n",
        "  return norm_preds, mse"
      ],
      "metadata": {
        "id": "LtPgN6oWXmgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop through Stocks"
      ],
      "metadata": {
        "id": "yCKKcHPrtWTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in Data\n",
        "prices_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/closing_prices.csv\")"
      ],
      "metadata": {
        "id": "2bBQ2Rn1FKsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "normalized_df = prices_df.copy()\n",
        "min_series = prices_df.iloc[:, 1:].min()\n",
        "max_series = prices_df.iloc[:, 1:].max()\n",
        "normalized_df.iloc[:, 1:] = (prices_df.iloc[:, 1:] - min_series) / (max_series - min_series)"
      ],
      "metadata": {
        "id": "8evPn7WAFPLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update each time\n",
        "last_pred_date = '1/2/2024'\n",
        "\n",
        "#Initial Split\n",
        "train_df = normalized_df[normalized_df['Date'] < last_pred_date]\n",
        "test_df = normalized_df[normalized_df['Date'] >= last_pred_date]\n",
        "\n",
        "#date range\n",
        "date_range = test_df['Date'].tolist()"
      ],
      "metadata": {
        "id": "gGLW9DuKs_K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intialize DataFrames - First Time Only\n",
        "norm_preds_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "mse_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])"
      ],
      "metadata": {
        "id": "3IBbYCSdtV2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop through all stocks\n",
        "for j in range(start_index, prices_df.shape[1]):\n",
        "  print(j)\n",
        "  #Load Model\n",
        "  model_filename = f'/content/drive/My Drive/HODL_Project/LSTM_Models/model_column_{j}.h5'\n",
        "  model = load_model(model_filename)\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "  #Series for jth stock\n",
        "  train_series = train_df.iloc[:,j]\n",
        "  test_series = test_df.iloc[:,j]\n",
        "\n",
        "  #Update the model\n",
        "  norm_preds, mse = update_model(model, train_series, test_series)\n",
        "\n",
        "  norm_preds_df = pd.concat([norm_preds_df, norm_preds], axis=1)\n",
        "  mse_df = pd.concat([mse_df, mse], axis=1)\n",
        "\n",
        "  #Save DataFrames\n",
        "  norm_preds_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/norm_preds.csv', index=False)\n",
        "  mse_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/mse.csv', index=False)\n"
      ],
      "metadata": {
        "id": "vObmEArkG_2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Once all stocks are have run - update columns\n",
        "prices_df_cols = list(prices_df.columns)\n",
        "norm_preds_df.columns = prices_df_cols\n",
        "mse_df.columns = prices_df_cols"
      ],
      "metadata": {
        "id": "vFoYnle7OBzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# De-normalize predictions\n",
        "preds_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "\n",
        "for j in range(1, norm_preds_df.shape[1]):\n",
        "  de_norm = norm_preds_df.iloc[:,j] * (max_series[j-1] - min_series[j-1]) + min_series[j-1]\n",
        "  preds_df = pd.concat([preds_df, de_norm], axis=1)\n",
        "\n",
        "#Rename columns\n",
        "preds_df.columns = prices_df_cols"
      ],
      "metadata": {
        "id": "oAm_DwFtObY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save csv with col names\n",
        "norm_preds_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/norm_preds.csv', index=False)\n",
        "preds_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/preds.csv', index=False)\n",
        "mse_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/mse.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Jj5_tcoxOfGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Measures"
      ],
      "metadata": {
        "id": "GrYg4oIWOjSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in Files if not already loaded - commented out because assumed loaded\n",
        "prices_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/closing_prices.csv\")\n",
        "norm_preds_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/norm_preds.csv\")\n",
        "preds_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/preds.csv\")\n",
        "mse_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/mse.csv\")\n",
        "\n",
        "#Normalize Data\n",
        "normalized_df = prices_df.copy()\n",
        "min_series = prices_df.iloc[:, 1:].min()\n",
        "max_series = prices_df.iloc[:, 1:].max()\n",
        "normalized_df.iloc[:, 1:] = (prices_df.iloc[:, 1:] - min_series) / (max_series - min_series)\n",
        "\n",
        "#Assure last_pred_date & price_df_cols is defined\n",
        "last_pred_date = pd.to_datetime('1/2/2024').tz_localize('UTC')\n",
        "prices_df_cols = list(prices_df.columns)"
      ],
      "metadata": {
        "id": "Q-Mr5YBivI68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get train DF\n",
        "test_df_norm = normalized_df[pd.to_datetime(normalized_df['Date']) > last_pred_date]\n",
        "test_df = prices_df[pd.to_datetime(prices_df['Date'])  > last_pred_date]"
      ],
      "metadata": {
        "id": "fPgJ9-ZLvEkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define date_range\n",
        "date_range = test_df['Date'].tolist()"
      ],
      "metadata": {
        "id": "QbLfKFQj5WTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_norm.head()"
      ],
      "metadata": {
        "id": "eP4xAU0V5jdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Absolute Error"
      ],
      "metadata": {
        "id": "mraRzb3kOx-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calc MAE for all stocks\n",
        "\n",
        "mae_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "\n",
        "for j in range(1, preds_df.shape[1]):\n",
        "  mae_list = []\n",
        "  for i in range(0, preds_df.shape[0]):\n",
        "    mae = abs(preds_df.iloc[i,j] - test_df.iloc[i,j])\n",
        "    mae_list.append(mae)\n",
        "\n",
        "  mae_df_j = pd.DataFrame(mae_list)\n",
        "  mae_df = pd.concat([mae_df, mae_df_j], axis=1)\n",
        "\n",
        "mae_df.columns = prices_df_cols"
      ],
      "metadata": {
        "id": "fyp_q4ncO4W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rolling MAE - does not include that date\n",
        "rolling_mae_7day  = pd.DataFrame(data=date_range[2:], columns = ['Date'])\n",
        "for j in range(1, mae_df.shape[1]):\n",
        "  col = mae_df.iloc[:,j].rolling(window=7).mean()\n",
        "  rolling_mae_7day = pd.concat([rolling_mae_7day, col], axis=1)\n",
        "\n",
        "rolling_mae_7day.columns = prices_df_cols\n",
        "rolling_mae_7day = rolling_mae_7day.iloc[:-1]"
      ],
      "metadata": {
        "id": "TjZaMb9cOgb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forecasted Change & MDE"
      ],
      "metadata": {
        "id": "qGK2OVbwO_rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initalize DFs\n",
        "#Forecasted Change\n",
        "delta_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "#Forecasted % Change\n",
        "delta_perc_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "#Actual Price Change\n",
        "actual_delta_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "#If actual and predicted change have same sign\n",
        "directional_error_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n"
      ],
      "metadata": {
        "id": "OesqT6jSPNuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(1, preds_df.shape[1]):\n",
        "  #Initialize Lists\n",
        "  delta_list = []\n",
        "  delta_perc_list = []\n",
        "  actual_delta_list = []\n",
        "  directional_error_list = []\n",
        "  for i in range(0, preds_df.shape[0]):\n",
        "    #Get forecasted change & % change\n",
        "    delta = preds_df.iloc[i,j] - test_df.iloc[i,j] #same i b/c test_df starts on 1/2 but pred_df on 1/3\n",
        "    delta_perc = delta / test_df.iloc[i,j]\n",
        "    #Append to list\n",
        "    delta_list.append(delta)\n",
        "    delta_perc_list.append(delta_perc)\n",
        "\n",
        "    #Get Actual Change\n",
        "    actual_delta = test_df.iloc[i+1,j] - test_df.iloc[i,j]\n",
        "    actual_delta_list.append(actual_delta)\n",
        "\n",
        "    #Directional Error\n",
        "    #Calc directional error : 0 if wrong 1 if right\n",
        "    if actual_delta > 0 and delta > 0:\n",
        "      directional_error = 1\n",
        "    elif actual_delta < 0 and delta < 0:\n",
        "      directional_error = 1\n",
        "    else:\n",
        "      directional_error = 0\n",
        "    directional_error_list.append(directional_error)\n",
        "\n",
        "  #Join dfs\n",
        "  delta_df = pd.concat([delta_df, pd.DataFrame(delta_list)], axis=1)\n",
        "  delta_perc_df = pd.concat([delta_perc_df, pd.DataFrame(delta_perc_list)], axis=1)\n",
        "  actual_delta_df = pd.concat([actual_delta_df, pd.DataFrame(actual_delta_list)], axis=1)\n",
        "  directional_error_df = pd.concat([directional_error_df, pd.DataFrame(directional_error_list)], axis=1)\n"
      ],
      "metadata": {
        "id": "04owkHliPTco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename Columns\n",
        "delta_df.columns = prices_df_cols\n",
        "delta_perc_df.columns = prices_df_cols\n",
        "actual_delta_df.columns = prices_df_cols\n",
        "directional_error_df.columns = prices_df_cols"
      ],
      "metadata": {
        "id": "V_HMftNZPY73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a 7 day rolling MDE (output is a percentage)\n",
        "rolling_mde_7day  = pd.DataFrame(data=date_range[2:], columns = ['Date'])\n",
        "for j in range(1, directional_error_df.shape[1]):\n",
        "  col = directional_error_df.iloc[:,j].rolling(window=7).mean() * 100\n",
        "  rolling_mde_7day = pd.concat([rolling_mde_7day, col], axis=1)\n",
        "\n",
        "rolling_mde_7day.columns = prices_df_cols\n",
        "rolling_mde_7day = rolling_mde_7day.iloc[:-1]"
      ],
      "metadata": {
        "id": "lYN0rWzgPdaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save dfs needs for agent"
      ],
      "metadata": {
        "id": "IjGToJu-0rRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/delta_df.csv', index = False)\n",
        "rolling_mde_7day.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/rolling_mde_7day.csv', index = False)\n",
        "delta_perc_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/delta_perc_df.csv', index = False)\n",
        "rolling_mae_7day.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/rolling_mae_7day.csv', index = False)"
      ],
      "metadata": {
        "id": "qHXRqsGg0RyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See how 1 stock performs DFs"
      ],
      "metadata": {
        "id": "UF8z_q42cvmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock = 'AAPL'\n",
        "stock_df = test_df[['Date', stock]]\n",
        "stock_df.rename(columns = {stock:'Actual'}, inplace = True)\n",
        "stock_df = stock_df.merge(preds_df[['Date', stock]],\n",
        "                        on = 'Date',\n",
        "                        how = 'inner')\n",
        "stock_df.rename(columns = {stock:'Predicted'}, inplace = True)"
      ],
      "metadata": {
        "id": "7C_gXqFXV22L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(pd.to_datetime(stock_df['Date']), stock_df['Actual'])\n",
        "plt.plot(pd.to_datetime(stock_df['Date']), stock_df['Predicted'])\n",
        "plt.legend(['Actual', 'Predicted'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.title(stock)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2cMgvfPYxmVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calc Baseline MSE & MAE"
      ],
      "metadata": {
        "id": "5DU-sX-txpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_mse_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])"
      ],
      "metadata": {
        "id": "qDwvo1kOxtOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(1, normalized_df.shape[1]):\n",
        "\n",
        "  mse_list = []\n",
        "\n",
        "  for i in range(1, normalized_df.shape[0]):\n",
        "    mse = np.square(normalized_df.iloc[i-1,j] - normalized_df.iloc[i,j])\n",
        "    mse_list.append(mse)\n",
        "\n",
        "  baseline_mse_df = pd.concat([baseline_mse_df, pd.DataFrame(data=mse_list, columns = [prices_df_cols[j]])], axis=1)"
      ],
      "metadata": {
        "id": "y97ZBFcixwpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_mae_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "\n",
        "for j in range(1, test_df.shape[1]):\n",
        "\n",
        "  mae_list = []\n",
        "\n",
        "  for i in range(1, test_df.shape[0]):\n",
        "    mae = abs(test_df.iloc[i,j] - test_df.iloc[i-1,j])\n",
        "    mae_list.append(mae)\n",
        "\n",
        "  baseline_mae_df = pd.concat([baseline_mae_df, pd.DataFrame(data=mae_list, columns = [prices_df_cols[j]])], axis=1)"
      ],
      "metadata": {
        "id": "Uq6CPPTsx3Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare NMSE to baseline"
      ],
      "metadata": {
        "id": "-guuG74xx9j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM\n",
        "mean_mse = mse_df.mean()\n",
        "average_mse_df = mean_mse.to_frame(name='Average MSE')\n",
        "\n",
        "# Reset the index to make the stock names a column\n",
        "average_mse_df.reset_index(inplace=True)\n",
        "average_mse_df.rename(columns={'index': 'Stock'}, inplace=True)"
      ],
      "metadata": {
        "id": "R8ceJ5nJyAs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make Histogram\n",
        "mse_avg_data = average_mse_df['Average MSE']\n",
        "mse_avg_data = np.clip(mse_avg_data, 0, 0.05)\n",
        "bins = np.arange(start=min(mse_avg_data), stop=0.05, step=0.0025)  # Adjust step for finer or coarser bins\n",
        "bins = np.append(bins, np.max(mse_avg_data))\n",
        "\n",
        "plt.hist(mse_avg_data, bins=bins, edgecolor='black')\n",
        "plt.title('LSTM: NMSE Distribution')\n",
        "plt.xlabel('Avg MSE')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Displaying the histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kyRp2_u0yPE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Baseline\n",
        "mean_baseline_mse = baseline_mse_df.mean()\n",
        "average_baseline_mse_df = mean_baseline_mse.to_frame(name='Average MSE')\n",
        "\n",
        "# Reset the index to make the stock names a column\n",
        "average_baseline_mse_df.reset_index(inplace=True)\n",
        "average_baseline_mse_df.rename(columns={'index': 'Stock'}, inplace=True)"
      ],
      "metadata": {
        "id": "q6pU26g4yBPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a histogram of performance\n",
        "b_mse_avg_data = average_baseline_mse_df['Average MSE']\n",
        "b_mse_avg_data = np.clip(b_mse_avg_data, 0, 0.05)\n",
        "bins = np.arange(start=min(b_mse_avg_data), stop=0.05, step=0.0025)  # Adjust step for finer or coarser bins\n",
        "#bins = np.append(bins, np.max(b_mse_avg_data))\n",
        "\n",
        "plt.hist(b_mse_avg_data, bins=bins, edgecolor='black')\n",
        "plt.title('Baseline: NMSE Distribution')\n",
        "plt.xlabel('Avg MSE')\n",
        "plt.ylabel('Companies')\n",
        "\n",
        "# Displaying the histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2PQDDhepyDS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine how many stocks the LSTM performs better for\n",
        "better = 0\n",
        "\n",
        "for i in range(1, len(mse_avg_data)):\n",
        "  if mse_avg_data[i] < b_mse_avg_data[i]:\n",
        "    better += 1\n",
        "\n",
        "print(better)"
      ],
      "metadata": {
        "id": "KojuXSTIyaDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare MAE to Baseline"
      ],
      "metadata": {
        "id": "KNNENtcHymMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calc Average Baseline MAE\n",
        "mean_baseline_mae = baseline_mae_df.mean()\n",
        "average_baseline_mae_df = mean_baseline_mae.to_frame(name='Baseline MAE')\n",
        "# Reset the index to make the stock names a column\n",
        "average_baseline_mae_df.reset_index(inplace=True)\n",
        "average_baseline_mae_df.rename(columns={'index': 'Stock'}, inplace=True)"
      ],
      "metadata": {
        "id": "Q9lq-v-1ylKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calc Average LSTM MAE\n",
        "mean_mae = mae_df.mean()\n",
        "average_mae_df = mean_mae.to_frame(name='Predicted MAE')\n",
        "average_mae_df.reset_index(inplace=True)\n",
        "average_mae_df.rename(columns={'index': 'Stock'}, inplace=True)"
      ],
      "metadata": {
        "id": "bRLpqadwytbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join the DFs\n",
        "average_mae_df = average_mae_df.merge(average_baseline_mae_df, on='Stock')"
      ],
      "metadata": {
        "id": "aHWpoftnyvop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calc difference\n",
        "average_mae_df['Dif'] = average_mae_df['Predicted MAE'] - average_mae_df['Baseline MAE']"
      ],
      "metadata": {
        "id": "wA0lqP1zy_3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine how many are negative (baseline performs better)\n",
        "average_mae_df['Negative Dif'] = average_mae_df['Dif'] < 0\n",
        "average_mae_df['Negative Dif'].sum()"
      ],
      "metadata": {
        "id": "0nt59thpzGIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Directional Error in 2024"
      ],
      "metadata": {
        "id": "7OFDh8GzzFnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_mde = directional_error_df.mean()\n",
        "average_mde_df = avg_mde.to_frame(name='Average MDE')\n",
        "\n",
        "# Reset the index to make the stock names a column\n",
        "average_mde_df.reset_index(inplace=True)\n",
        "average_mde_df.rename(columns={'index': 'Stock'}, inplace=True)"
      ],
      "metadata": {
        "id": "4jrYBB-8zS0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Average across all stocks\n",
        "average_mde_df['Average MDE'].mean()"
      ],
      "metadata": {
        "id": "CmFsySfczX-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make Histogram\n",
        "b_mde_avg_data = average_mde_df['Average MDE']\n",
        "bins = np.arange(start=0, stop=1, step=0.1)  # Adjust step for finer or coarser bins\n",
        "#bins = np.append(bins, np.max(b_mse_avg_data))\n",
        "\n",
        "plt.hist(b_mde_avg_data, bins=bins, edgecolor='black')\n",
        "plt.title('LSTM: MDE Distribution')\n",
        "plt.xlabel('Avg MDE')\n",
        "plt.ylabel('Companies')\n",
        "\n",
        "# Displaying the histogram\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x0UbbsSozYAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Garch Model Evaluation"
      ],
      "metadata": {
        "id": "VCocf_UQbVyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/closing_prices.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], utc = True)\n",
        "df"
      ],
      "metadata": {
        "id": "W5CvN442rfK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series0 = df[(df['Date'].dt.year == 2023) | (df['Date'].dt.year == 2022)]\n",
        "#467(2022-02-22 to 2023-12-29) * 498(Date + 497 Stocks)\n",
        "\n",
        "df_2024 = df[df['Date'].dt.year == 2024]\n",
        "#35 rows(index:467-501, 2024-01-02 to 2024-02-21)  498 columns"
      ],
      "metadata": {
        "id": "7bjSXfQRilsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_range = df[df['Date'] >= '2024-01-02']['Date']\n",
        "date_range = date_range.tolist()\n",
        "len(date_range)"
      ],
      "metadata": {
        "id": "WE0Ig2iMm1gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialize dataframes for storing predicted prices, MSEs, and volatilities (only run once)"
      ],
      "metadata": {
        "id": "aLF8h-HfpLV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_names = list(df.columns)[1:]\n",
        "num_rows = 36\n",
        "\n",
        "preds_df = pd.DataFrame(columns=['Date'])\n",
        "preds_df['Date'] = date_range\n",
        "mse_df = pd.DataFrame(columns=['Date'])\n",
        "mse_df['Date'] = date_range\n",
        "volatility_df = pd.DataFrame(columns=['Date'])\n",
        "volatility_df['Date'] = date_range\n",
        "\n",
        "for stock_name in stock_names:\n",
        "    preds_df[stock_name] = [None] * (num_rows-1)\n",
        "    mse_df[stock_name] = [None] * (num_rows-1)\n",
        "    volatility_df[stock_name] = [None] * (num_rows-1)\n",
        "#35 rows  498 columns\n",
        "\n",
        "# Save the initialized DataFrames to CSV files (only run once)\n",
        "#preds_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/garch_preds.csv', index=False)\n",
        "#mse_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/garch_mse.csv', index=False)\n",
        "#volatility_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/garch_volatility.csv', index=False)"
      ],
      "metadata": {
        "id": "M1cZnPnrrzDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculate log returns\n",
        "shifts all the values in the DataFrame df except for the first column (iloc[:, 1:]) down by one row.\n",
        "\n",
        "\n",
        "*   Each value in the DataFrame is moved down by one row.\n",
        "*  The first row becomes NaN (missing value) because there's no value to shift into its place.\n",
        "*   The last row is discarded because it's shifted out of the DataFrame's range.\n"
      ],
      "metadata": {
        "id": "_vTnqMTTpu0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_returns(df):\n",
        "  #skip the 0th col (the Date col), subset from the 1st col\n",
        "  log_returns = np.log(df.iloc[:, 1:] / df.iloc[:, 1:].shift(1))\n",
        "  log_returns = log_returns.dropna()\n",
        "  return log_returns"
      ],
      "metadata": {
        "id": "CRJxXa3xo3sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For all stocks from 2024-01-02 to 2024-02-21"
      ],
      "metadata": {
        "id": "3pKt9JP1RC_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_preds.csv\")\n",
        "mse_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_mse.csv\")\n",
        "volatility_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_volatility.csv\")"
      ],
      "metadata": {
        "id": "dq_SUchDx6Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#base_save_path = '/content/drive/My Drive/HODL_Project/Updated_GARCH/'\n",
        "for stock in range(0, 497):\n",
        "  mse_column = []  #list to store MSE values for the current stock\n",
        "  next_day_price_pred_column = []  #list to store next day price predictions for the current stock\n",
        "  volatility_column = []\n",
        "  for time in range(0, 35):\n",
        "    #when time=0: initial models\n",
        "    series_df = pd.concat([series0, df_2024.head(time)])\n",
        "    series_log_returns = log_returns(series_df).iloc[:,stock]\n",
        "\n",
        "    # Fit a GARCH(1,1) model\n",
        "    #uses the most recent observation and the previous observation to calculate the conditional variance at each time step.\n",
        "    #effective rolling window size is 2.\n",
        "    model = arch_model(series_log_returns, vol='Garch', p=1, q=1, mean='constant', dist='Normal', rescale=False)\n",
        "    results = model.fit(disp='off')\n",
        "\n",
        "    ###########################################################################\n",
        "    # Predict the next day's returns\n",
        "    pred_returns = results.forecast(horizon=1).mean.iloc[-1].values[0]\n",
        "\n",
        "    # extract the last available/known price\n",
        "    last_available_price = series_df.iloc[-1,stock+1]\n",
        "\n",
        "    # Use the last available price to predict the next day's price\n",
        "    next_day_price_pred = last_available_price * np.exp(pred_returns)\n",
        "    next_day_price_pred_column.append(next_day_price_pred)\n",
        "\n",
        "    # Calculate mean squared error (MSE) for price prediction\n",
        "    actual_next_day_price = df_2024.iloc[time,stock+1]\n",
        "    price_mse = (next_day_price_pred - actual_next_day_price) ** 2\n",
        "    mse_column.append(price_mse)\n",
        "    ###########################################################################\n",
        "    # Extract the forecasted conditional variance\n",
        "    forecasted_conditional_variance = results.forecast(horizon=1).variance.iloc[-1]\n",
        "    # Calculate volatility as the square root of conditional variance\n",
        "    volatility = np.sqrt(forecasted_conditional_variance)\n",
        "    volatility_column.append(volatility)\n",
        "    ###########################################################################\n",
        "    # # Construct the save path for the model\n",
        "    # model_save_path = os.path.join(base_save_path, f'garch_model_stock{stock+1}_day{time+1}.pkl')\n",
        "\n",
        "    # # Save the model using pickle\n",
        "    # with open(model_save_path, 'wb') as f:\n",
        "    #     pickle.dump(results, f)\n",
        "\n",
        "    # print(f\"Model saved to {model_save_path}\")\n",
        "    ###########################################################################\n",
        "    #print(f'Stock{stock+1}_Day{time+1}')\n",
        "\n",
        "  # Add the next day price predictions and MSE values to the corresponding cells in the DataFrames\n",
        "  preds_df.iloc[:, stock+1] = next_day_price_pred_column\n",
        "  mse_df.iloc[:, stock+1] = mse_column\n",
        "  volatility_df.iloc[:, stock+1] = volatility_column\n",
        "  print(f'Stock{stock+1}')\n",
        "\n",
        "# Save the updated DataFrames back to the CSV files\n",
        "#preds_df.to_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_preds.csv\", index=False)\n",
        "#mse_df.to_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_mse.csv\", index=False)\n",
        "#volatility_df.to_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_volatility.csv\", index=False)"
      ],
      "metadata": {
        "id": "9eVzYrOC41AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^^\n",
        "\n",
        "Takes ~15min to run with model model saving\n",
        "\n",
        "Takes ~7min to run without model saving"
      ],
      "metadata": {
        "id": "p7JBexUsXcJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots - predicted vs. actual price"
      ],
      "metadata": {
        "id": "mywNVtlT0Ojm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_preds.csv\")"
      ],
      "metadata": {
        "id": "t38JkjZmJeY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGL\n",
        "actual_prices = df_2024['GOOGL'].tolist()\n",
        "predicted_prices = preds_df['GOOGL'].tolist()\n",
        "dates = date_range\n",
        "\n",
        "dates = pd.to_datetime(dates)\n",
        "\n",
        "plt.plot(dates, actual_prices, label='Actual Prices')\n",
        "plt.plot(dates, predicted_prices, label='Predicted Prices')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Actual vs Predicted Prices for GOOGL')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dZtZ05wg9LRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AMZN\n",
        "actual_prices = df_2024['AMZN'].tolist()\n",
        "predicted_prices = preds_df['AMZN'].tolist()\n",
        "dates = date_range\n",
        "\n",
        "dates = pd.to_datetime(dates)\n",
        "\n",
        "plt.plot(dates, actual_prices, label='Actual Prices')\n",
        "plt.plot(dates, predicted_prices, label='Predicted Prices')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Actual vs Predicted Prices for AMZN')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2L0m0W4S_6Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BKNG\n",
        "actual_prices = df_2024['BKNG'].tolist()\n",
        "predicted_prices = preds_df['BKNG'].tolist()\n",
        "dates = date_range\n",
        "\n",
        "dates = pd.to_datetime(dates)\n",
        "\n",
        "plt.plot(dates, actual_prices, label='Actual Prices')\n",
        "plt.plot(dates, predicted_prices, label='Predicted Prices')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Actual vs Predicted Prices for BKNG')\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8smu9awMAMZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots - predicted vs. actual volatility\n",
        "Volatility: measure of the amount of uncertainty or risk involved in the size of changes in a stock's value.\n",
        "\n",
        "* Calculate actual volatility (in 2024) from the historical closing prices data\n",
        "\n",
        "  * uses the most recent observation and the previous observation to calculate the conditional variance\n",
        "\n",
        "  * effective rolling window size = 2"
      ],
      "metadata": {
        "id": "4wa6VeU7FhPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#subset historical closing prices to start from one day before the first day in 2024\n",
        "closing_prices_2024 = pd.concat([series0.tail(2), df_2024], ignore_index=True)\n",
        "\n",
        "#(don't need to drop the first \"Date\" col because the log_returns(df) function handles that)\n",
        "\n",
        "#calculate log returns (using function defined earlier)\n",
        "calculated_log_returns = log_returns(closing_prices_2024)\n",
        "\n",
        "# Calculate rolling variance of log returns using only previous day info\n",
        "rolling_variance = calculated_log_returns.rolling(window=2).var()\n",
        "\n",
        "# Calculate volatility as the square root of rolling variance\n",
        "actual_volatility = np.sqrt(rolling_variance)"
      ],
      "metadata": {
        "id": "WhY2aeAdujsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the first row which becomes NaNs after calculating the log_returns\n",
        "actual_volatility = actual_volatility.dropna()\n",
        "\n",
        "# Add a new row at the end with NaN values (referring to the volatility on the last day)\n",
        "#actual_volatility.loc[len(actual_volatility)] = [np.nan] * len(actual_volatility.columns)\n",
        "\n",
        "# Insert the \"Date\" column at the beginning of the DataFrame\n",
        "actual_volatility.insert(0, \"Date\", date_range)\n",
        "\n",
        "#actual_volatility.to_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_actual_volatility.csv\", index=False)"
      ],
      "metadata": {
        "id": "3kSB2SO0JADe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volatility_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_volatility.csv\")\n",
        "actual_volatility = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_actual_volatility.csv\")\n",
        "\n",
        "date_range = df[df['Date'] >= '2024-01-02']['Date']\n",
        "date_range = date_range.tolist()\n",
        "\n",
        "dates = date_range\n",
        "dates = pd.to_datetime(dates)"
      ],
      "metadata": {
        "id": "Ks8M11DMQKQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGL\n",
        "pred_vol = volatility_df['GOOGL'].tolist()\n",
        "actual_vol = actual_volatility['GOOGL'].tolist()\n",
        "\n",
        "plt.plot(dates, pred_vol, label='Predicted Volatility')\n",
        "plt.plot(dates, actual_vol, label='Actual Volatility')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volatility')\n",
        "plt.title('Predicted and Actual Volatility for GOOGL')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hjK00L4FFuIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AMZN\n",
        "pred_vol_amzn = volatility_df['AMZN'].tolist()\n",
        "actual_vol_amzn = actual_volatility['AMZN'].tolist()\n",
        "\n",
        "plt.plot(dates, pred_vol_amzn, label='Predicted Volatility')\n",
        "plt.plot(dates, actual_vol_amzn, label='Actual Volatility')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volatility')\n",
        "plt.title('Predicted and Actual Volatility for AMZN')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2MECVhV0SB8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BKNG\n",
        "pred_vol_bkng = volatility_df['BKNG'].tolist()\n",
        "actual_vol_bkng = actual_volatility['BKNG'].tolist()\n",
        "\n",
        "plt.plot(dates, pred_vol_bkng, label='Predicted Volatility')\n",
        "plt.plot(dates, actual_vol_bkng, label='Actual Volatility')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volatility')\n",
        "plt.title('Predicted and Actual Volatility for BKNG')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MHcX-sH2Sl6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AAPL\n",
        "pred_vol_aapl = volatility_df['AAPL'].tolist()\n",
        "actual_vol_aapl = actual_volatility['AAPL'].tolist()\n",
        "\n",
        "plt.plot(dates, pred_vol_aapl, label='Predicted Volatility')\n",
        "plt.plot(dates, actual_vol_aapl, label='Actual Volatility')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volatility')\n",
        "plt.title('Predicted and Actual Volatility for AAPL')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJ2Y_2XaejDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output for Agent"
      ],
      "metadata": {
        "id": "aRAnnIhKbcOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Company Names (Wikipedia)"
      ],
      "metadata": {
        "id": "sPmRpGj_bjcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to Get company name\n",
        "def fetch_sp500_info():\n",
        "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "    tables = pd.read_html(url)  # This parses all the tables in webpages\n",
        "    sp500_table = tables[0]  # Assuming the first table is the S&P 500 list\n",
        "    sp500_table = sp500_table[['Symbol', 'Security']]  # Adjust column name if necessary\n",
        "    return sp500_table"
      ],
      "metadata": {
        "id": "9SSJoq8ybgxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save company name in a table\n",
        "sp500_table = fetch_sp500_info()"
      ],
      "metadata": {
        "id": "qqp4Puckbgzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make sure all data is present"
      ],
      "metadata": {
        "id": "BSlldYsLz3cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in all data (if needed)\n",
        "prices_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/closing_prices.csv\")\n",
        "actual_vol_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_actual_volatility.csv\")\n",
        "pred_vol_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/garch_volatility.csv\")\n",
        "preds_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/Prediction_DF/preds.csv\")\n",
        "delta_df = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/delta_df.csv')\n",
        "rolling_mde_7day = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/rolling_mde_7day.csv')\n",
        "delta_perc_df = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/delta_perc_df.csv')\n",
        "rolling_mae_7day = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/rolling_mae_7day.csv')"
      ],
      "metadata": {
        "id": "yj3EEOLEz6VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calc Garch Model % Change"
      ],
      "metadata": {
        "id": "2ac_aE-o7XOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change in Volitility DF - Actual starts 1/2, pred starts 1/3\n",
        "#Check calculation is right\n",
        "perc_change_pred_vol_df = pd.DataFrame(data=date_range[1:], columns = ['Date'])\n",
        "\n",
        "for j in range(1, pred_vol_df.shape[1]):\n",
        "  vol_change_list = []\n",
        "  for i in range(0, pred_vol_df.shape[0]-1):\n",
        "    val = (pred_vol_df.iloc[i+1,j] - actual_vol_df.iloc[i,j]) / actual_vol_df.iloc[i,j]\n",
        "    vol_change_list.append(val)\n",
        "  col = pd.DataFrame(vol_change_list)\n",
        "  perc_change_pred_vol_df = pd.concat([perc_change_pred_vol_df, col], axis=1)\n",
        "\n",
        "perc_change_pred_vol_df.columns = prices_df_cols"
      ],
      "metadata": {
        "id": "_yX395aE7Wmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Relevant Rows from DFs"
      ],
      "metadata": {
        "id": "JFKf5Y29bvTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Last Day Predictions\n",
        "\n",
        "#Last actual - 2 days before:\n",
        "last_price = prices_df.iloc[-2,1:]\n",
        "last_actual_vol = actual_vol_df.iloc[-2,1:]\n",
        "\n",
        "#Preds - Last Row\n",
        "next_pred = preds_df.iloc[-1,1:]\n",
        "last_pred_vol = pred_vol_df.iloc[-1,1:]\n",
        "\n",
        "#Change - Last Row\n",
        "last_pred_change = delta_df.iloc[-1,1:]\n",
        "last_pred_change_perc = delta_perc_df.iloc[-1,1:]\n",
        "last_perc_vol_perc = perc_change_pred_vol_df.iloc[-1,1:]  * 100\n",
        "\n",
        "#Rolling Errors - Last Row based on code design (excludes the day itself)\n",
        "last_avg_mse = rolling_mae_7day.iloc[-1,1:]\n",
        "last_avg_mde = rolling_mde_7day.iloc[-1,1:]"
      ],
      "metadata": {
        "id": "QAm6kSYxbg15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make DF"
      ],
      "metadata": {
        "id": "f_5i-E5ZcBph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### May need to update based on Yutong's Code #####\n",
        "#Make Dictionary\n",
        "output_df = {'Ticker' : prices_df_cols[1:],\n",
        "             'current_price': last_price,\n",
        "             'forecasted_price' : next_pred,\n",
        "             'forecasted_price_change':last_pred_change,\n",
        "             'percent_change': last_pred_change_perc * 100,\n",
        "             'current_volatility': last_actual_vol,\n",
        "             'forecasted_volatility': last_pred_vol,\n",
        "             'forecasted_volatility_change': last_perc_vol_perc,\n",
        "             'model_error_last_week' : last_avg_mse,\n",
        "             'model_mda_last_week':last_avg_mde}\n",
        "\n",
        "#Make DF\n",
        "output_df = pd.DataFrame(data=output_df)"
      ],
      "metadata": {
        "id": "wtYknBUnb_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join with SP500 table for company name\n",
        "output_df = output_df.merge(sp500_table, left_on='Ticker', right_on = 'Symbol', how='left')"
      ],
      "metadata": {
        "id": "y-MFYi0JcGWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reorder cols so company name is second\n",
        "cols = list(output_df.columns)\n",
        "new_order = [cols[0]] + [cols[-1]] + cols[1:-1]\n",
        "output_df = output_df[new_order]\n",
        "output_df.head()"
      ],
      "metadata": {
        "id": "GQBDcbSMcRbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = output_df.drop(columns=['Symbol'])\n",
        "output_df = output_df.rename(columns={'Security': 'Stock'})"
      ],
      "metadata": {
        "id": "ai7uwi7Icf9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df['Ticker'] = output_df['Ticker'].str.lower()\n",
        "output_df['Stock'] = output_df['Stock'].str.lower()\n",
        "output_df.tail()"
      ],
      "metadata": {
        "id": "FVj7OENx1LTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Output\n",
        "output_df.to_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/output_df.csv', index=False)"
      ],
      "metadata": {
        "id": "zwBLWXWvcmeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slot Filing"
      ],
      "metadata": {
        "id": "xiPaWgwvmonk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to Github: xxx"
      ],
      "metadata": {
        "id": "0l7Cn6oZmxDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the \"HODL\" library\n",
        "!wget -q 'https://www.dropbox.com/s/4rdgil1epnvgitf/HODL.py'"
      ],
      "metadata": {
        "id": "PFCxjJXnnPLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from HODL import TransformerEncoder, PositionalEmbedding\n",
        "\n",
        "##################### CONSTANTS #####################\n",
        "keras.utils.set_random_seed(2024)\n",
        "MAX_QUERY_LENGTH = 50  #size of input\n",
        "EMBED_DIM = 512  #dimension of embeddings\n",
        "DENSE_DIM = 128\n",
        "NUM_HEADS = 8  #number of multi-attention heads\n",
        "DENSE_UNITS = 128  #num nodes in hidden layer\n",
        "BATCH_SIZE = 64  #batch size for training transformer\n",
        "EPOCHS = 10  #epochs for training transformer\n",
        "\n",
        "# Read training data\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/testing_data.csv\")\n",
        "train_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/training_data.csv\")\n",
        "\n",
        "train_query = train_df['query'].values\n",
        "train_slotfilling = train_df['slot filling'].values\n",
        "test_query = test_df['query'].values\n",
        "test_slotfilling = test_df['slot filling'].values\n",
        "#####################################################\n",
        "\n",
        "# CREATE VECTORIZER (QUERY & SLOTS)\n",
        "vectorize_query_text = keras.layers.TextVectorization(\n",
        "    max_tokens=None,  #no maximum vocabulary\n",
        "    output_sequence_length=MAX_QUERY_LENGTH,  #pad or truncate output to value\n",
        "    output_mode=\"int\",  #vector has index of vocabulary\n",
        "    standardize=\"lower_and_strip_punctuation\",  #convert input to lowercase and rmv punctuation\n",
        "    split=\"whitespace\",  #split values based on whitespace\n",
        "    ngrams=1  #only look at whole words\n",
        ")\n",
        "vectorize_slot_text = keras.layers.TextVectorization(\n",
        "    max_tokens=None,  #no maximum vocabulary\n",
        "    output_sequence_length=MAX_QUERY_LENGTH,\n",
        "    output_mode=\"int\",  #vector has index of vocabulary\n",
        "    standardize=\"lower\",  #convert input to lowercase [can't do punctuation b/c of dashes]\n",
        "    split=\"whitespace\",  #split values based on whitespace\n",
        "    ngrams=1  #only look at whole words\n",
        ")\n",
        "\n",
        "# CREATE VOCABULARY AND VECTORIZED TRAINING DATA\n",
        "vectorize_query_text.adapt(train_query)  #build vocabulary\n",
        "query_train = vectorize_query_text(train_query)  #vectorized training queries\n",
        "query_test = vectorize_query_text(test_query)  #vectorized testing queries\n",
        "QUERY_VOCAB_SIZE = vectorize_query_text.vocabulary_size() #total vocabulary of queries\n",
        "\n",
        "vectorize_slot_text.adapt(train_slotfilling)  #build slot vocabulary\n",
        "slots_train = vectorize_slot_text(train_slotfilling)  #vectorized training slots\n",
        "slots_test = vectorize_slot_text(test_slotfilling)  #vectorized testing slots\n",
        "SLOT_VOCAB_SIZE = vectorize_slot_text.vocabulary_size()  #total vocabulary of slots\n",
        "\n",
        "# BUILD KERAS MODEL\n",
        "inputs = keras.Input(shape=(MAX_QUERY_LENGTH,))\n",
        "embedding = PositionalEmbedding(MAX_QUERY_LENGTH,\n",
        "                                QUERY_VOCAB_SIZE,\n",
        "                                EMBED_DIM)\n",
        "x = embedding(inputs)\n",
        "encoder_out = TransformerEncoder(EMBED_DIM,\n",
        "                                 DENSE_DIM,\n",
        "                                 NUM_HEADS)(x)\n",
        "x = keras.layers.Dense(DENSE_UNITS, activation=\"relu\", name=\"Dense_Layer\")(encoder_out)\n",
        "x = keras.layers.Dropout(0.25, name=\"Dropout_Layer\")(x)\n",
        "outputs = keras.layers.Dense(SLOT_VOCAB_SIZE, activation=\"softmax\", name=\"Softmax_Layer\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "print()\n",
        "print(model.summary())\n",
        "print()\n",
        "\n",
        "# TRAIN KERAS MODEL\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"sparse_categorical_accuracy\"])\n",
        "history = model.fit(query_train, slots_train,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=EPOCHS)\n",
        "\n",
        "# OUT-OF-SAMPLE TESTING\n",
        "model.evaluate(query_test, slots_test)\n",
        "\n",
        "# SAVE MODEL\n",
        "filename = '/content/drive/My Drive/HODL_Project/sql_transformer.keras'\n",
        "model.save(filename)\n",
        "ZipFile('/content/drive/My Drive/HODL_Project/model_save.zip', mode='w').write(filename)\n",
        "\n",
        "# EVALUATING SLOT ACCURACY\n",
        "def slot_filling_accuracy(actual, predicted, only_slots=False):\n",
        "    not_padding = np.not_equal(actual, 0) #+ np.not_equal(predicted, 0)\n",
        "\n",
        "    if only_slots:\n",
        "        non_slot_token = vectorize_slot_text(['O']).numpy()[0, 0]\n",
        "        slots = np.not_equal(actual, non_slot_token)\n",
        "        correct_predictions = np.equal(actual, predicted)[not_padding * slots]\n",
        "    else:\n",
        "        correct_predictions = np.equal(actual, predicted)[not_padding]\n",
        "\n",
        "    sample_length = len(correct_predictions)\n",
        "\n",
        "    weights = np.ones(sample_length)\n",
        "\n",
        "    return np.dot(correct_predictions, weights) / sample_length\n",
        "\n",
        "predicted = np.argmax(model.predict(query_test), axis=-1).reshape(-1)\n",
        "actual = slots_test.numpy().reshape(-1)\n",
        "\n",
        "acc = slot_filling_accuracy(actual, predicted, only_slots=False)\n",
        "acc_slots = slot_filling_accuracy(actual, predicted, only_slots=True)\n",
        "\n",
        "print(f'Accuracy = {acc:.3f}')\n",
        "print(f'Accuracy on slots = {acc_slots:.3f}')\n",
        "\n",
        "# TEST-SET EVALUATION\n",
        "def predict_slots_query(query, model, query_vectorizer, slot_vectorizer):\n",
        "    sentence = query_vectorizer([query])\n",
        "\n",
        "    prediction = np.argmax(model.predict(sentence), axis=-1)[0]\n",
        "\n",
        "    inverse_vocab = dict(enumerate(slot_vectorizer.get_vocabulary()))\n",
        "    decoded_prediction = \" \".join(inverse_vocab[int(i)] for i in prediction)\n",
        "    return decoded_prediction\n",
        "\n",
        "for example, answer in zip(test_query, test_slotfilling):\n",
        "    print()\n",
        "    print(\"Query:\\n\", example)\n",
        "    print(\"Answer:\\n\", answer)\n",
        "    print(\"Prediction:\\n\", predict_slots_query(example,\n",
        "                                               model,\n",
        "                                               vectorize_query_text,\n",
        "                                               vectorize_slot_text))\n",
        "    print()"
      ],
      "metadata": {
        "id": "4a6G6oa_m-qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slot Parser Function"
      ],
      "metadata": {
        "id": "cGqViqcWnnsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SlotParser(slot_filling, prompt, stock_data):\n",
        "    slot_filling = slot_filling.strip()  #strip any whitespace from slot_filling return\n",
        "    # initialize aspects of the SQL query\n",
        "    slots = {'select': [],\n",
        "             'order': [],\n",
        "             'limit': None}\n",
        "\n",
        "    # for word (token) in prompt\n",
        "    for word in prompt.split(\" \"):\n",
        "        if word.isnumeric():  #if we found a number\n",
        "            slots['limit'] = int(word)  #assume number relates to LIMIT (# of rows to display)\n",
        "\n",
        "    # for slot (token) in output\n",
        "    for token in slot_filling.split(\" \"):\n",
        "        if (token != \"o\") and (token != \"O\"):  #if not a null slot\n",
        "            if 'select-' in token:  #if a select slot\n",
        "                if token not in slots['select']:  #if we haven't already added it\n",
        "                    slots['select'].append(token)  #then include in SELECT statement\n",
        "            elif 'order-by' in token:  #if an order-by slot\n",
        "                if token not in slots['order']:  #if we haven't already added it\n",
        "                    slots['order'].append(token)  #then include in ORDER BY statement\n",
        "\n",
        "    if len(slots['select']) == 0:  #if we haven't selected a colummn\n",
        "        columns = list(stock_data.columns)  #assume all\n",
        "    else:  #if we have selected >= 1 column\n",
        "        columns = [x.split(\"-\")[1] for x in slots['select']]  #access column names, format them\n",
        "\n",
        "    if len(slots['order']) == 0:  #if we haven't ordered a column\n",
        "        if 'stock' not in columns:\n",
        "            columns.insert(0, 'stock')  #insert at beginning\n",
        "        order_cols = ['stock']  #assume we order by stock\n",
        "        order_ascending = [True]  #assum ascending order\n",
        "    else:  #if there is order statement\n",
        "        order_cols = [x.split(\"-\")[2] for x in slots['order']]  #format\n",
        "        order_ascending = [True if x.split(\"-\")[-1] == 'asc' else False for x in slots['order']]  #format\n",
        "\n",
        "    if slots['limit'] == None:  #if we don't have a number\n",
        "        limit = len(stock_data)  #assume we want all rows\n",
        "    else:  #if there is a limit number\n",
        "        limit = int(slots['limit'])  #make sure to return that many rows\n",
        "\n",
        "    for col in order_cols:  #for every ordering column\n",
        "        if col not in columns:  #check if it is an accessible column\n",
        "            columns.append(col)  #if not, add it\n",
        "\n",
        "    pandas_query = stock_data[columns]  #use only selected columns\n",
        "    pandas_query = pandas_query.sort_values(by=order_cols,  #columns to sort\n",
        "                                            ascending=order_ascending,  #boolean list of asc/desc\n",
        "                                            ignore_index=True)\n",
        "    pandas_query = pandas_query.head(limit)  #LIMIT statement\n",
        "\n",
        "    all_data = stock_data.sort_values(by=order_cols,\n",
        "                                      ascending=order_ascending,\n",
        "                                      ignore_index=True)  #include just in case\n",
        "    # print(pandas_query)  #print the dataframe\n",
        "\n",
        "    #formatting SQL statements\n",
        "    SELECT = f\"SELECT {', '.join(columns)}\"\n",
        "    FROM = \"FROM stock_data\"\n",
        "    ORDER_BY = f\"ORDER BY {', '.join((str(x.split('-')[2])+' '+str(x.split('-')[-1].upper())) for x in slots['order'])}\"\n",
        "    LIMIT = f\"LIMIT {slots['limit']}\"\n",
        "\n",
        "    #checking for errors\n",
        "    if len(slots['select']) == 0:  #empty slots\n",
        "        SQL_QUERY = \"SELECT *\\n FROM stock_data\"\n",
        "    elif (len(slots['order']) == 0) and (slots['limit'] == None):  #no sort or limit statement\n",
        "        SQL_QUERY = f\"{SELECT}\\n{FROM}\"\n",
        "    elif len(slots['order']) == 0:  #no order clause\n",
        "        SQL_QUERY = f\"{SELECT}\\n{FROM}\\n{LIMIT}\"\n",
        "    elif slots['limit'] == None:  #no limit clause\n",
        "        SQL_QUERY = f\"{SELECT}\\n{FROM}\\n{ORDER_BY}\"\n",
        "    else:\n",
        "        SQL_QUERY = f\"{SELECT}\\n{FROM}\\n{ORDER_BY}\\n{LIMIT}\"\n",
        "\n",
        "    print(SQL_QUERY)  #print SQL statement\n",
        "\n",
        "    return pandas_query, SQL_QUERY, all_data"
      ],
      "metadata": {
        "id": "AC5p5vrvnnhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slot Filling Evaluation"
      ],
      "metadata": {
        "id": "nmSyHTsEnc2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_slots = [\n",
        "    \"o select-stock o o o o o order-by-forecasted_price_change-desc select-forecasted_price o o o o\",\n",
        "    \"o select-stock o select-forecasted_volatility o select-forecasted_volatility o o o order-by-forecasted_volatility-desc select-forecasted_volatility o o o\",\n",
        "    \"o select-stock o o o o o o o order-by-percent_change-asc select-percent_change select-percent_change o o\",\n",
        "    \"o select-ticker o o o o o order-by-forecasted_price-desc select-forecasted_price o order-by-forecasted_volatility-asc select-forecasted_volatility\"\n",
        "]\n",
        "example_prompts = [\n",
        "    \"What 5 stocks are expected to have the highest increase in price for tomorrow?\",\n",
        "    \"What 10 stocks are forecasted or predicted to have the highest volatility in their price?\",\n",
        "    \"What 2 stocks are forecasted or predicted to have the lowest percent change in price?\",\n",
        "    \"What 8 tickers are predicted to have the highest price and lowest volatility\"\n",
        "]\n",
        "data = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/output_df.csv')\n",
        "\n",
        "for (slot, prompt) in zip(example_slots, example_prompts):\n",
        "    print(slot)\n",
        "    print(prompt)\n",
        "    SlotParser(slot, prompt, data)\n",
        "    print()"
      ],
      "metadata": {
        "id": "WVnh2s0qna7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Application"
      ],
      "metadata": {
        "id": "sIlokWH6nY2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from zipfile import ZipFile\n",
        "import re\n",
        "\n",
        "from HODL import TransformerEncoder, PositionalEmbedding\n",
        "\n",
        "##################### CONSTANTS #####################\n",
        "keras.utils.set_random_seed(2024)\n",
        "MAX_QUERY_LENGTH = 50  #size of input\n",
        "\n",
        "# Read training data\n",
        "train_df = pd.read_csv(\"/content/drive/My Drive/HODL_Project/training_data.csv\")\n",
        "\n",
        "train_query = train_df['query'].values\n",
        "train_slotfilling = train_df['slot filling'].values\n",
        "\n",
        "transformer_model = keras.models.load_model(\"/content/drive/My Drive/HODL_Project/sql_transformer.keras\", custom_objects={\n",
        "    \"TransformerEncoder\": TransformerEncoder,\n",
        "    \"PositionalEmbedding\": PositionalEmbedding,\n",
        "})\n",
        "\n",
        "\n",
        "stock_data = pd.read_csv('/content/drive/My Drive/HODL_Project/Prediction_DF/output_df.csv')\n",
        "#####################################################\n",
        "\n",
        "# CREATE VECTORIZER (QUERY & SLOTS)\n",
        "vectorize_query_text = keras.layers.TextVectorization(\n",
        "    max_tokens=None,  #no maximum vocabulary\n",
        "    output_sequence_length=MAX_QUERY_LENGTH,  #pad or truncate output to value\n",
        "    output_mode=\"int\",  #vector has index of vocabulary\n",
        "    standardize=\"lower_and_strip_punctuation\",  #convert input to lowercase and rmv punctuation\n",
        "    split=\"whitespace\",  #split values based on whitespace\n",
        "    ngrams=1  #only look at whole words\n",
        ")\n",
        "vectorize_slot_text = keras.layers.TextVectorization(\n",
        "    max_tokens=None,  #no maximum vocabulary\n",
        "    output_sequence_length=MAX_QUERY_LENGTH,\n",
        "    output_mode=\"int\",  #vector has index of vocabulary\n",
        "    standardize=\"lower\",  #convert input to lowercase [can't do punctuation b/c of dashes]\n",
        "    split=\"whitespace\",  #split values based on whitespace\n",
        "    ngrams=1  #only look at whole words\n",
        ")\n",
        "\n",
        "# CREATE VOCABULARY AND VECTORIZED TRAINING DATA\n",
        "vectorize_query_text.adapt(train_query)  #build vocabulary\n",
        "query_train = vectorize_query_text(train_query)  #vectorized training queries\n",
        "\n",
        "vectorize_slot_text.adapt(train_slotfilling)  #build slot vocabulary\n",
        "slots_train = vectorize_slot_text(train_slotfilling)  #vectorized training slots\n",
        "\n",
        "#### BEGIN STREAMLIT APPLICATION ####\n",
        "st.title(\"Day-Trader GPT\")\n",
        "st.write()\n",
        "st.write(\"Enter your question into the following textbox:\")\n",
        "\n",
        "prompt = st.text_area(label=\"Prompt:\",\n",
        "                      value=\"\")\n",
        "run_query_button = st.button(\"Run query\",\n",
        "                             type=\"primary\")\n",
        "\n",
        "if run_query_button:\n",
        "    formatted_prompt = re.sub(r'[0-9]', '', prompt)\n",
        "    slot_filling = predict_slots_query(formatted_prompt,\n",
        "                                    transformer_model,\n",
        "                                    vectorize_query_text,\n",
        "                                    vectorize_slot_text)\n",
        "\n",
        "    return_df, written_query, all_data = SlotParser(slot_filling,\n",
        "                                                    prompt,\n",
        "                                                    stock_data)\n",
        "    return_df.columns = [\" \".join([x.capitalize() for x in col.split(\"_\")]) for col in return_df.columns]\n",
        "    st.data_editor(return_df,\n",
        "                hide_index=True,\n",
        "                key=\"filtered_df\",\n",
        "                use_container_width=True,\n",
        "                disabled=True)\n",
        "\n",
        "    st.write(\"Prompt you submitted:\")\n",
        "    st.info(prompt)\n",
        "\n",
        "    st.write(\"Slot filling procedure (removes number requests):\")\n",
        "    st.info(slot_filling)\n",
        "\n",
        "    st.write(\"SQL Query:\")\n",
        "    st.info(written_query)\n",
        "\n",
        "    st.write(\"Entire Table:\")\n",
        "    st.data_editor(all_data,\n",
        "                hide_index=True,\n",
        "                use_container_width=True,\n",
        "                disabled=True)"
      ],
      "metadata": {
        "id": "0kXM7nSFnTMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}